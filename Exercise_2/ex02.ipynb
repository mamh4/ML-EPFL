{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)# x here is just a vector to be standardized (subtract mean and divide by std dev)\n",
    "y, tx = build_model_data(x, weight) # tx here is a matrix of x and 1s (for intercept) where x here represents the feature matrix (DxN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.94406149]\n",
      " [ 1.          0.62753668]\n",
      " [ 1.          2.01244346]\n",
      " ...\n",
      " [ 1.         -0.64968792]\n",
      " [ 1.          0.69312469]\n",
      " [ 1.         -1.14970831]]\n"
     ]
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "\n",
    "## 1.1. Define the Cost Function\n",
    "\n",
    "\\begin{equation}\n",
    "    MSE=\\frac{1}{2N} \\sum_{n=1}^{N} \\underbrace{[y_n-x_n^Tw]^2}_{L_n(w) = e_n^2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    MAE=\\frac{1}{N} \\sum_{n=1}^{N} |y_n-x_n^Tw|\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2694.4833658870843"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_loss(y, tx, w,method='mse'):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    e = y - tx.dot(w)\n",
    "    if method == 'mse':\n",
    "        return 1/2*np.mean(e**2)\n",
    "    elif method == 'mae':\n",
    "        return np.mean(abs(e))\n",
    "        \n",
    "    # ***************************************************\n",
    "\n",
    "#compute_loss(weight,tx,np.array([1,2]),method='mae')\n",
    "compute_loss(weight,tx,np.array([1,2]),method='mse')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    for i, w0 in enumerate(grid_w0):\n",
    "        for j, w1 in enumerate(grid_w1):\n",
    "            losses[i,j] = compute_loss(y,tx,np.array([w0,w1]),method='mse')\n",
    "\n",
    "\n",
    "    # ***************************************************\n",
    "    return losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=17.365057211881126, w0*=74.74747474747477, w1*=12.121212121212125, execution time=0.690 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=100)#Num of intervals at 10 results in bad results\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26.706078  ,  6.52028757])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "\n",
    "    # ***************************************************\n",
    "    return -tx.T.dot(y-tx.dot(w))/len(y)\n",
    "\n",
    "#compute_gradient(weight,tx,np.array([50,10]))\n",
    "compute_gradient(weight,tx,np.array([100,20]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):#This is the gradient using the >>MSE<< loss function\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w,method='mse')\n",
    "        \n",
    "        w = w - gamma*gradient  \n",
    "        # ***************************************************\n",
    "        # store w and loss    \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=73.29392200210518, w0=51.305745401473644, w1=9.435798704492269\n",
      "GD iter. 1/49: loss=21.988886592353907, w0=66.69746902191571, w1=12.266538315840005\n",
      "GD iter. 2/49: loss=7.265302114604107, w0=71.31498610804834, w1=13.115760199244333\n",
      "GD iter. 3/49: loss=4.699945676696628, w0=72.70024123388814, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=4.447409765322356, w0=73.11581777164007, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=4.426368339890847, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=4.424908541814049, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=4.42494113435403, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=4.424986760372248, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=4.425002951094766, w0=73.29348920882515, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=4.42500808460945, w0=73.29379216412117, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=4.425009648774414, w0=73.29388305070998, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=4.425010118023903, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=4.425010258798749, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=4.425010301031204, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=4.425010313700939, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=4.425010317501861, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=4.425010318642136, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=4.425010318984219, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=4.425010319086845, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=4.425010319117633, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=4.425010319126868, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=4.4250103191296395, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=4.425010319130471, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=4.4250103191307195, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=4.425010319130795, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=4.425010319130818, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=4.425010319130824, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=4.425010319130825, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=4.425010319130827, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=4.425010319130827, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.027 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca499a628f142959aed54cfc45f36e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    stochastic_gradient = tx.T.dot(tx.dot(w) - y) * 1 / len(y) #So far exactly as standard gradient.\n",
    "    # ***************************************************\n",
    "    return stochastic_gradient\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "        # Perform operations or computations on minibatch_y and minibatch_tx\n",
    "            w = w - gamma * compute_stoch_gradient(minibatch_y,minibatch_tx,w)\n",
    "            loss = compute_loss(minibatch_y,minibatch_tx,w,method='mse')\n",
    "        # ***************************************************\n",
    "\n",
    "        # store w and loss    \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=1467.9891465026938, w0=6.069692838753188, w1=-1.638977261551186\n",
      "SGD iter. 1/49: loss=1944.3635954147496, w0=12.998607069465763, w1=-1.5762365094563813\n",
      "SGD iter. 2/49: loss=1669.1013682351381, w0=19.540973951436406, w1=1.1114314459820913\n",
      "SGD iter. 3/49: loss=712.0267789036076, w0=27.095603950575157, w1=16.229817646736542\n",
      "SGD iter. 4/49: loss=478.1737224073174, w0=31.676446258539784, w1=9.359948380399786\n",
      "SGD iter. 5/49: loss=341.62692604381556, w0=35.16532327793744, w1=5.075771279237409\n",
      "SGD iter. 6/49: loss=489.64517060602606, w0=38.642393545922886, w1=5.06799554265962\n",
      "SGD iter. 7/49: loss=792.9899841099526, w0=43.256005159980816, w1=7.867024096101708\n",
      "SGD iter. 8/49: loss=518.7147258554764, w0=47.04842115769055, w1=10.567259235319089\n",
      "SGD iter. 9/49: loss=265.9361542867602, w0=49.62105411526534, w1=10.08250835737919\n",
      "SGD iter. 10/49: loss=419.679234633569, w0=52.8918597560524, w1=11.316517540318419\n",
      "SGD iter. 11/49: loss=41.24955669957683, w0=55.33425131490527, w1=16.929314075015437\n",
      "SGD iter. 12/49: loss=7.589249405236791, w0=55.878426659076666, w1=17.667593329665667\n",
      "SGD iter. 13/49: loss=180.41603621409044, w0=57.98959747476559, w1=17.769892291872555\n",
      "SGD iter. 14/49: loss=103.10626839012767, w0=59.705624190149976, w1=16.40592378577926\n",
      "SGD iter. 15/49: loss=86.65555022827355, w0=61.21629239992742, w1=15.5987866389821\n",
      "SGD iter. 16/49: loss=41.93777132656948, w0=62.25369421431229, w1=15.168758381123359\n",
      "SGD iter. 17/49: loss=46.947869047436875, w0=63.64039575199406, w1=13.201689905739453\n",
      "SGD iter. 18/49: loss=35.6592923645802, w0=65.08062663366518, w1=15.752295208067881\n",
      "SGD iter. 19/49: loss=13.726565835763795, w0=65.73264461446888, w1=15.112099287574798\n",
      "SGD iter. 20/49: loss=0.04381858287002741, w0=65.76807298747903, w1=15.083665829741967\n",
      "SGD iter. 21/49: loss=65.8924760038205, w0=67.07924557372353, w1=14.435119926263255\n",
      "SGD iter. 22/49: loss=25.139797982281827, w0=67.93140392717301, w1=13.732929646775482\n",
      "SGD iter. 23/49: loss=22.885362788366145, w0=68.68311613033003, w1=13.731356086258184\n",
      "SGD iter. 24/49: loss=0.4427377156998881, w0=68.51552535454445, w1=13.423009545784337\n",
      "SGD iter. 25/49: loss=23.16158646816076, w0=69.27177244561634, w1=13.41385118861537\n",
      "SGD iter. 26/49: loss=6.631963724572403, w0=69.67709820007575, w1=13.463018967583679\n",
      "SGD iter. 27/49: loss=120.54158454950534, w0=71.40580620125283, w1=13.229594704484334\n",
      "SGD iter. 28/49: loss=1.0377187673218458, w0=71.24106445473595, w1=13.146375806075858\n",
      "SGD iter. 29/49: loss=1.9867101948899366, w0=71.46686959218844, w1=13.24010050952454\n",
      "SGD iter. 30/49: loss=1.8609383182845682, w0=71.25077269311795, w1=13.298264984697706\n",
      "SGD iter. 31/49: loss=11.881428487897159, w0=71.80094478773658, w1=13.50386469376554\n",
      "SGD iter. 32/49: loss=0.4063124012192412, w0=71.91341388226951, w1=13.392250996083773\n",
      "SGD iter. 33/49: loss=11.0927001869947, w0=71.38905726531621, w1=13.461219877744298\n",
      "SGD iter. 34/49: loss=0.01214442822355297, w0=71.36334772582806, w1=13.41715149622446\n",
      "SGD iter. 35/49: loss=7.229805543464186, w0=71.82933206997616, w1=13.84415284547134\n",
      "SGD iter. 36/49: loss=0.09808019356632752, w0=71.77935181390389, w1=13.862753379047131\n",
      "SGD iter. 37/49: loss=29.834469945450145, w0=72.67352535534941, w1=14.400166819447577\n",
      "SGD iter. 38/49: loss=1.8727827329293512, w0=72.89775424021603, w1=14.53635339383121\n",
      "SGD iter. 39/49: loss=5.540914828689066, w0=73.29869966456927, w1=14.871155654038448\n",
      "SGD iter. 40/49: loss=10.14721387849417, w0=72.67217373068863, w1=15.713981205931734\n",
      "SGD iter. 41/49: loss=0.8217819238997074, w0=72.82475274044704, w1=15.596021355969068\n",
      "SGD iter. 42/49: loss=6.3913691232384755, w0=72.42749028280123, w1=15.601163980640457\n",
      "SGD iter. 43/49: loss=21.664322840948845, w0=71.51122391938416, w1=14.366409318287166\n",
      "SGD iter. 44/49: loss=0.5606406619004916, w0=71.64220434681948, w1=14.24108203664467\n",
      "SGD iter. 45/49: loss=0.2552209628546978, w0=71.56232602041571, w1=14.259941473782021\n",
      "SGD iter. 46/49: loss=0.0002546860452907113, w0=71.56494799902465, w1=14.261583665917408\n",
      "SGD iter. 47/49: loss=9.893870925851893, w0=72.13285125848083, w1=13.64806969886073\n",
      "SGD iter. 48/49: loss=31.425975165749822, w0=73.07331811313884, w1=14.358247910014361\n",
      "SGD iter. 49/49: loss=1.5001225541490584, w0=73.3362218947703, w1=14.766517767413871\n",
      "SGD: execution time=0.041 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad3fe09745a4fd88cb12681aaeb4bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# Load data bs sub-sampling and outliers\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "GD iter. 1/49: loss=318.2821247015965, w0=67.40170332798297, w1=10.041754328050114\n",
      "GD iter. 2/49: loss=88.6423556165128, w0=72.06797509684336, w1=10.736952704607411\n",
      "GD iter. 3/49: loss=67.9747763988552, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.0516072257859, w1=11.032481534481914\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536945\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260336, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260336, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166\n",
      "GD iter. 25/49: loss=65.93073010260336, w0=74.06780585492449, w1=11.034894865988822\n",
      "GD iter. 26/49: loss=65.93073010260336, w0=74.06780585492581, w1=11.034894865989015\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.012 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "#       and the model fit\n",
    "ws = [w_initial]\n",
    "w = w_initial\n",
    "losses = []\n",
    "for n_iter in range(max_iters):\n",
    "    gradient = compute_gradient(y,tx,w)\n",
    "    loss = compute_loss(y,tx,w,method='mse')\n",
    "    w = w - gamma*gradient  \n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "          bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607eb0060fb7448d85beed2f27e4c39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w): #Notice that earlier we only changed the loss compuation but the gradient was always using the MSE loss function\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    e = y - tx.dot(w)\n",
    "    e[np.where(e>0)] = 1\n",
    "    e[np.where(e<0)] = -1\n",
    "    e[np.where(e==0)] = 0.3 # or any other value between -1 and 1\n",
    "    return -tx.T.dot(e)/len(y)\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        sub_gradient = compute_subgradient_mae(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y,tx,w,method='mae')\n",
    "        w = w - gamma*sub_gradient\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=55.867805854926395, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492639, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492639, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=30.66780585492635, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=27.17327020966892, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=26.490451563751197, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=23.899295346035593, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=23.284392925657144, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=22.686876444181845, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=21.537818828008433, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=19.91191015895785, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=19.389644090563234, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=18.887989064395885, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=18.415960501854236, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=17.954898543040386, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=17.505757656579824, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=17.07495742693161, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=16.652967297509903, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=16.24854073149673, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=15.849105212654159, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=15.46691979123133, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=15.108294621512215, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=14.754896345922832, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=14.40452896162028, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=14.055787028127279, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=13.714620911605635, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=13.381236307284155, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=13.058821615166238, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=12.74025172433924, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=12.42321888875611, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=12.107561731901173, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=11.800622097398135, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=11.495041794646427, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=11.189461491894715, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=10.883881189143004, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=10.584593408313202, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=10.295816534318941, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=10.01135208122136, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=9.72808432666813, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=9.44812546112251, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=9.17104110409667, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=8.903656131158964, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=8.636271158221257, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=8.376151920302375, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=8.140540838751496, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=7.918544501597273, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=7.705279728377, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=7.493695831178641, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=7.289992405743416, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=7.097234035781543, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=6.919905294668923, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=6.750573527315454, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=6.584744810805664, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=6.430343276347806, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=6.278071481890353, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=6.133663329263324, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=6.00584079834303, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=5.885021825223219, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=5.771635252269658, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=5.667162061790257, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=5.586726765993146, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=5.523847812160388, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=5.480093708591872, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=5.4530880035020255, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=5.427392630862905, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=5.407322445682752, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=5.387252260502599, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=5.3704607803386955, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=5.357406523334741, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=5.345929264022584, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=5.335714659517473, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=5.330043910465361, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=5.325676428273225, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=5.322176726526591, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=5.320111309643114, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=5.3172400485651465, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=5.315557122666144, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=5.313876880922167, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=5.3130522468713846, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=5.312377839024387, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=5.312132229725043, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=5.311683566098433, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=5.311594306869985, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=5.311505047641534, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=5.311393473605972, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=5.311125695920624, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=5.311081066306399, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=5.310823570190169, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=5.310804671438473, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=5.310749731161021, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=5.310733434318958, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=5.310717105690679, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=5.310684480220337, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=5.310662165413225, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=5.310606458729927, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=5.310613573874789, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=5.310588318629318, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=5.310620689019651, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=5.3105749661498844, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=5.310622915165495, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=5.310626930749847, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=5.31057969173614, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=5.310580796439088, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=5.310624267896667, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=5.310577675701808, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=5.310576117459501, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=5.310581714323563, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=5.310623831189332, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=5.310577035343975, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=5.310623394481999, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=5.31057869984858, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=5.310576683814244, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=5.310625183920505, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=5.310584467976984, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=5.3106225210673275, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=5.310576022555871, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=5.3106260999443435, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=5.310625663237008, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=5.310575030668312, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=5.310623000383831, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=5.310578781555702, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=5.310622563676499, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=5.31062478982234, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=5.3105780708494175, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=5.3106270159681825, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=5.310579756254566, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=5.310626142553512, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=5.31057674833267, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=5.310578691998486, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=5.310584288862546, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=5.310574930903372, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=5.310624395724173, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=5.310626621870017, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=5.31057945788459, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=5.310623959016839, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=5.310576766672319, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=5.310582363536381, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=5.310625311748012, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=5.31057952032574, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=5.310580438210215, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=5.310624438333343, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=5.310575759230627, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=5.310626664479185, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=5.3105794901438035, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=5.310575458075135, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=5.310582273979162, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=5.3105775949995735, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=5.310625791064514, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=5.310623128211338, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=5.310578512884048, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=5.31058410974811, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=5.310624917649847, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=5.310574751788934, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=5.310580348652994, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=5.3105756696734066, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=5.31057658755788, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=5.310626270381019, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=5.310582184421943, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=5.310577505442355, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=5.310575159705162, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=5.310578423326827, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=5.3105840201908885, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=5.310576514481122, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=5.310624960259014, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=5.310627186404858, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=5.310579885291417, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=5.310624523551678, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=5.3105764980006605, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=5.3105830127491975, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=5.310625439575518, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=5.310583930633672, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.122 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a24260fc015453ba3f85a80c980009b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            w = w - gamma * compute_subgradient_mae(minibatch_y,minibatch_tx,w)\n",
    "            loss = compute_loss(minibatch_y,minibatch_tx,w,method='mae')\n",
    "\n",
    "\n",
    "        # store w and loss    \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=60.01062926494484, w0=0.7, w1=-0.6037660699665137\n",
      "SubSGD iter. 1/499: loss=84.60421559734024, w0=1.4, w1=-0.17624898474909562\n",
      "SubSGD iter. 2/499: loss=48.85174001849255, w0=2.0999999999999996, w1=-0.9949073545547238\n",
      "SubSGD iter. 3/499: loss=87.9632135535922, w0=2.8, w1=-0.3802463734243371\n",
      "SubSGD iter. 4/499: loss=50.27417247729759, w0=3.5, w1=-1.0050624732352833\n",
      "SubSGD iter. 5/499: loss=50.357281099853346, w0=4.2, w1=-1.5570339051729625\n",
      "SubSGD iter. 6/499: loss=86.53557895148033, w0=4.9, w1=-0.9335457828867849\n",
      "SubSGD iter. 7/499: loss=84.56798606603505, w0=5.6000000000000005, w1=-0.2873985494995642\n",
      "SubSGD iter. 8/499: loss=50.88370583370345, w0=6.300000000000001, w1=-0.5588787314821434\n",
      "SubSGD iter. 9/499: loss=56.36298107484033, w0=7.000000000000001, w1=-0.7407425750573122\n",
      "SubSGD iter. 10/499: loss=83.43476783146221, w0=7.700000000000001, w1=-0.3927755286668975\n",
      "SubSGD iter. 11/499: loss=35.505296601386945, w0=8.4, w1=-1.5187079421680683\n",
      "SubSGD iter. 12/499: loss=46.97699997286908, w0=9.1, w1=-2.0237256739923906\n",
      "SubSGD iter. 13/499: loss=68.99287162040547, w0=9.799999999999999, w1=-1.9973530233675696\n",
      "SubSGD iter. 14/499: loss=61.1657613715488, w0=10.499999999999998, w1=-2.1243674685955947\n",
      "SubSGD iter. 15/499: loss=70.35574226177822, w0=11.199999999999998, w1=-1.9589146766629688\n",
      "SubSGD iter. 16/499: loss=69.74406672618791, w0=11.899999999999997, w1=-1.6152020554795783\n",
      "SubSGD iter. 17/499: loss=44.29905481932042, w0=12.599999999999996, w1=-1.7350938689679094\n",
      "SubSGD iter. 18/499: loss=72.76267418406047, w0=13.299999999999995, w1=-0.7769587780661024\n",
      "SubSGD iter. 19/499: loss=62.13887276047039, w0=13.999999999999995, w1=-0.22856427430829152\n",
      "SubSGD iter. 20/499: loss=32.81706772282744, w0=14.699999999999994, w1=-1.4399871622344715\n",
      "SubSGD iter. 21/499: loss=84.53070207880876, w0=15.399999999999993, w1=-0.3216769416229901\n",
      "SubSGD iter. 22/499: loss=47.39195354083759, w0=16.099999999999994, w1=-0.4721336180741176\n",
      "SubSGD iter. 23/499: loss=40.72385234738576, w0=16.799999999999994, w1=-0.9990906538429655\n",
      "SubSGD iter. 24/499: loss=56.390423061530115, w0=17.499999999999993, w1=-0.9469303083132549\n",
      "SubSGD iter. 25/499: loss=42.05311499918602, w0=18.199999999999992, w1=-1.164187312493005\n",
      "SubSGD iter. 26/499: loss=65.41833707307424, w0=18.89999999999999, w1=-0.5596552066388458\n",
      "SubSGD iter. 27/499: loss=45.98240350975906, w0=19.59999999999999, w1=-0.9255759373692432\n",
      "SubSGD iter. 28/499: loss=72.09716689139734, w0=20.29999999999999, w1=-0.38461888539810096\n",
      "SubSGD iter. 29/499: loss=42.478424918452205, w0=20.99999999999999, w1=-0.5350755618492284\n",
      "SubSGD iter. 30/499: loss=38.93948854013739, w0=21.69999999999999, w1=-0.9305123537922344\n",
      "SubSGD iter. 31/499: loss=57.04663502049942, w0=22.399999999999988, w1=-0.8932250421743647\n",
      "SubSGD iter. 32/499: loss=33.911335122463434, w0=23.099999999999987, w1=-1.178399536723659\n",
      "SubSGD iter. 33/499: loss=32.45673437467606, w0=23.799999999999986, w1=-1.6680830701202474\n",
      "SubSGD iter. 34/499: loss=33.37847648566259, w0=24.499999999999986, w1=-1.9480120625567106\n",
      "SubSGD iter. 35/499: loss=35.431381020227796, w0=25.199999999999985, w1=-2.3790227473053243\n",
      "SubSGD iter. 36/499: loss=55.71593287155382, w0=25.899999999999984, w1=-2.2135699553726984\n",
      "SubSGD iter. 37/499: loss=30.4932684229923, w0=26.599999999999984, w1=-2.5988540957008266\n",
      "SubSGD iter. 38/499: loss=45.5111716967706, w0=27.299999999999983, w1=-2.617433554599703\n",
      "SubSGD iter. 39/499: loss=15.382880690814943, w0=27.999999999999982, w1=-3.8288564425258835\n",
      "SubSGD iter. 40/499: loss=66.98953846838944, w0=28.69999999999998, w1=-6.599156139414873\n",
      "SubSGD iter. 41/499: loss=7.092095158503959, w0=29.39999999999998, w1=-7.810579027341054\n",
      "SubSGD iter. 42/499: loss=31.54198129904374, w0=30.09999999999998, w1=-8.337767666088281\n",
      "SubSGD iter. 43/499: loss=28.450152374087693, w0=30.79999999999998, w1=-8.35152286206377\n",
      "SubSGD iter. 44/499: loss=9.857732056266265, w0=31.49999999999998, w1=-9.159636603105703\n",
      "SubSGD iter. 45/499: loss=4.567343088877976, w0=32.19999999999998, w1=-9.99567546037852\n",
      "SubSGD iter. 46/499: loss=68.89260279253294, w0=32.899999999999984, w1=-9.154485292153877\n",
      "SubSGD iter. 47/499: loss=32.03061893745372, w0=33.59999999999999, w1=-9.55046153537571\n",
      "SubSGD iter. 48/499: loss=70.91385562208929, w0=34.29999999999999, w1=-8.49406030399682\n",
      "SubSGD iter. 49/499: loss=37.56567778546446, w0=34.99999999999999, w1=-8.52875116333829\n",
      "SubSGD iter. 50/499: loss=54.662272503918985, w0=35.699999999999996, w1=-7.936433202070112\n",
      "SubSGD iter. 51/499: loss=25.326237803296017, w0=36.4, w1=-8.302353932800509\n",
      "SubSGD iter. 52/499: loss=47.2976630048175, w0=37.1, w1=-8.03221548251084\n",
      "SubSGD iter. 53/499: loss=31.017412099383023, w0=37.800000000000004, w1=-8.414341467061174\n",
      "SubSGD iter. 54/499: loss=15.531843059101078, w0=38.50000000000001, w1=-8.685821649043753\n",
      "SubSGD iter. 55/499: loss=43.134839802496614, w0=39.20000000000001, w1=-8.137427145285942\n",
      "SubSGD iter. 56/499: loss=28.0249791699285, w0=39.90000000000001, w1=-8.536884917672522\n",
      "SubSGD iter. 57/499: loss=38.842979860454065, w0=40.600000000000016, w1=-8.371078658105025\n",
      "SubSGD iter. 58/499: loss=49.24233307568128, w0=41.30000000000002, w1=-7.766546552250866\n",
      "SubSGD iter. 59/499: loss=39.614660644607106, w0=42.00000000000002, w1=-7.218152048493056\n",
      "SubSGD iter. 60/499: loss=21.9223304733965, w0=42.700000000000024, w1=-7.243449813881897\n",
      "SubSGD iter. 61/499: loss=46.168493304509695, w0=43.40000000000003, w1=-6.638917708027738\n",
      "SubSGD iter. 62/499: loss=20.543263808325428, w0=44.10000000000003, w1=-6.664215473416579\n",
      "SubSGD iter. 63/499: loss=23.965673245212542, w0=44.80000000000003, w1=-7.063673245803158\n",
      "SubSGD iter. 64/499: loss=2.832330122743521, w0=44.10000000000003, w1=-6.227634388530342\n",
      "SubSGD iter. 65/499: loss=50.302251497339114, w0=44.80000000000003, w1=-5.466277437911273\n",
      "SubSGD iter. 66/499: loss=52.695611394903196, w0=45.500000000000036, w1=-4.734312844403639\n",
      "SubSGD iter. 67/499: loss=11.374856762746155, w0=46.20000000000004, w1=-5.081396830023629\n",
      "SubSGD iter. 68/499: loss=22.438207975233432, w0=46.90000000000004, w1=-5.3368287111252\n",
      "SubSGD iter. 69/499: loss=29.968071371239304, w0=47.600000000000044, w1=-5.248301197636074\n",
      "SubSGD iter. 70/499: loss=55.75843786870038, w0=48.30000000000005, w1=-4.235862859769654\n",
      "SubSGD iter. 71/499: loss=11.734841839779413, w0=49.00000000000005, w1=-4.820175397315545\n",
      "SubSGD iter. 72/499: loss=29.36715773071038, w0=49.70000000000005, w1=-4.591053705028787\n",
      "SubSGD iter. 73/499: loss=32.53649444727173, w0=50.400000000000055, w1=-4.247341083845396\n",
      "SubSGD iter. 74/499: loss=62.38336009231787, w0=51.10000000000006, w1=-3.0665363831455004\n",
      "SubSGD iter. 75/499: loss=2.9684257780109107, w0=50.400000000000055, w1=-2.177704558979007\n",
      "SubSGD iter. 76/499: loss=41.68422428942084, w0=51.10000000000006, w1=-1.5347748602152114\n",
      "SubSGD iter. 77/499: loss=39.72947793004075, w0=51.80000000000006, w1=-1.1868078138247966\n",
      "SubSGD iter. 78/499: loss=10.080194844363675, w0=52.500000000000064, w1=-1.130498198779493\n",
      "SubSGD iter. 79/499: loss=6.19177723690332, w0=53.20000000000007, w1=-1.1442533947549838\n",
      "SubSGD iter. 80/499: loss=37.255846769091164, w0=53.90000000000007, w1=-0.3030632265303407\n",
      "SubSGD iter. 81/499: loss=40.978726210249285, w0=54.60000000000007, w1=0.5761847393658313\n",
      "SubSGD iter. 82/499: loss=7.13837565579523, w0=55.300000000000075, w1=0.6324943544111348\n",
      "SubSGD iter. 83/499: loss=31.605897878622628, w0=56.00000000000008, w1=1.3575145444919388\n",
      "SubSGD iter. 84/499: loss=23.315640182513043, w0=56.70000000000008, w1=1.7012271656753293\n",
      "SubSGD iter. 85/499: loss=2.14151977831456, w0=56.00000000000008, w1=2.542645675401274\n",
      "SubSGD iter. 86/499: loss=30.259226789610516, w0=56.70000000000008, w1=3.1887929087884945\n",
      "SubSGD iter. 87/499: loss=26.287953082893743, w0=57.400000000000084, w1=3.6163099940059125\n",
      "SubSGD iter. 88/499: loss=3.048853929641531, w0=58.10000000000009, w1=3.1930705692333983\n",
      "SubSGD iter. 89/499: loss=25.306194819958563, w0=58.80000000000009, w1=3.4585542714576287\n",
      "SubSGD iter. 90/499: loss=15.802354407203161, w0=59.50000000000009, w1=3.426125263690269\n",
      "SubSGD iter. 91/499: loss=22.464354395556775, w0=60.200000000000095, w1=3.876305009896563\n",
      "SubSGD iter. 92/499: loss=14.418205124667239, w0=60.9000000000001, w1=3.494179025346228\n",
      "SubSGD iter. 93/499: loss=0.22226086592575456, w0=60.200000000000095, w1=3.9991967571705502\n",
      "SubSGD iter. 94/499: loss=4.422209273950472, w0=60.9000000000001, w1=3.6289286666417535\n",
      "SubSGD iter. 95/499: loss=6.899703398251418, w0=61.6000000000001, w1=3.275812915074439\n",
      "SubSGD iter. 96/499: loss=14.117205526612317, w0=62.300000000000104, w1=3.5049346073611973\n",
      "SubSGD iter. 97/499: loss=3.4287368141954815, w0=61.6000000000001, w1=4.716357495287378\n",
      "SubSGD iter. 98/499: loss=3.417253394640774, w0=60.9000000000001, w1=5.557776005013323\n",
      "SubSGD iter. 99/499: loss=23.49843684958718, w0=61.6000000000001, w1=6.1812641272995\n",
      "SubSGD iter. 100/499: loss=0.16950141469060753, w0=62.300000000000104, w1=5.38689478280872\n",
      "SubSGD iter. 101/499: loss=22.640011954312925, w0=63.00000000000011, w1=6.592911072579167\n",
      "SubSGD iter. 102/499: loss=0.20808885752636996, w0=62.300000000000104, w1=7.776913272973058\n",
      "SubSGD iter. 103/499: loss=16.17528957272644, w0=63.00000000000011, w1=8.522898926560455\n",
      "SubSGD iter. 104/499: loss=14.373530419332411, w0=63.70000000000011, w1=9.052599900622571\n",
      "SubSGD iter. 105/499: loss=20.286074626583314, w0=64.4000000000001, w1=9.593556952593714\n",
      "SubSGD iter. 106/499: loss=11.58577124041328, w0=65.10000000000011, w1=10.121612768104363\n",
      "SubSGD iter. 107/499: loss=10.487424177135253, w0=65.80000000000011, w1=10.649668583615012\n",
      "SubSGD iter. 108/499: loss=12.145909754900089, w0=66.50000000000011, w1=10.250210811228433\n",
      "SubSGD iter. 109/499: loss=9.579121658449324, w0=67.20000000000012, w1=10.265518477940569\n",
      "SubSGD iter. 110/499: loss=8.537587947084589, w0=67.90000000000012, w1=10.408849068535085\n",
      "SubSGD iter. 111/499: loss=6.488621813401423, w0=68.60000000000012, w1=10.611285958576085\n",
      "SubSGD iter. 112/499: loss=4.14815329862256, w0=69.30000000000013, w1=10.721585992036877\n",
      "SubSGD iter. 113/499: loss=8.686949708848672, w0=70.00000000000013, w1=10.322128219650297\n",
      "SubSGD iter. 114/499: loss=5.76286980770152, w0=70.70000000000013, w1=11.068113873237694\n",
      "SubSGD iter. 115/499: loss=5.218745116922953, w0=70.00000000000013, w1=11.47918905777992\n",
      "SubSGD iter. 116/499: loss=20.258111120295467, w0=70.70000000000013, w1=12.71850535875792\n",
      "SubSGD iter. 117/499: loss=0.1521061736862137, w0=70.00000000000013, w1=13.339556932328513\n",
      "SubSGD iter. 118/499: loss=1.3626357534101459, w0=70.70000000000013, w1=13.58983804879057\n",
      "SubSGD iter. 119/499: loss=9.220767090309451, w0=71.40000000000013, w1=13.207712064240235\n",
      "SubSGD iter. 120/499: loss=5.534998888740574, w0=70.70000000000013, w1=12.60230891279625\n",
      "SubSGD iter. 121/499: loss=4.994079223310479, w0=70.00000000000013, w1=13.14947121503012\n",
      "SubSGD iter. 122/499: loss=12.848331108150475, w0=70.70000000000013, w1=13.589395657914302\n",
      "SubSGD iter. 123/499: loss=8.3194351142633, w0=71.40000000000013, w1=13.239457726433985\n",
      "SubSGD iter. 124/499: loss=1.1315535018089378, w0=72.10000000000014, w1=13.220878267535108\n",
      "SubSGD iter. 125/499: loss=1.450749828658985, w0=71.40000000000013, w1=12.563695564340122\n",
      "SubSGD iter. 126/499: loss=1.491085394363175, w0=70.70000000000013, w1=13.747697764734014\n",
      "SubSGD iter. 127/499: loss=2.6390922137082953, w0=71.40000000000013, w1=14.361612178111596\n",
      "SubSGD iter. 128/499: loss=0.3624066114551425, w0=72.10000000000014, w1=13.384814699169677\n",
      "SubSGD iter. 129/499: loss=0.1982252298454057, w0=71.40000000000013, w1=13.342488142566186\n",
      "SubSGD iter. 130/499: loss=8.385740195388848, w0=72.10000000000014, w1=12.960362158015851\n",
      "SubSGD iter. 131/499: loss=9.445622975219592, w0=72.80000000000014, w1=13.171526552582055\n",
      "SubSGD iter. 132/499: loss=13.606966320975062, w0=73.50000000000014, w1=13.515994799452523\n",
      "SubSGD iter. 133/499: loss=3.1773060580613475, w0=72.80000000000014, w1=13.955683204537708\n",
      "SubSGD iter. 134/499: loss=1.573185085360123, w0=72.10000000000014, w1=13.050508101350644\n",
      "SubSGD iter. 135/499: loss=2.639111269443582, w0=71.40000000000013, w1=13.44594489329365\n",
      "SubSGD iter. 136/499: loss=0.3250163759442728, w0=70.70000000000013, w1=14.230177621332974\n",
      "SubSGD iter. 137/499: loss=3.315620606705444, w0=70.00000000000013, w1=14.380634297784102\n",
      "SubSGD iter. 138/499: loss=6.1998450845480875, w0=69.30000000000013, w1=15.061233475440185\n",
      "SubSGD iter. 139/499: loss=1.0485807234944104, w0=68.60000000000012, w1=15.764738393831047\n",
      "SubSGD iter. 140/499: loss=7.645018737986085, w0=69.30000000000013, w1=15.509306512729477\n",
      "SubSGD iter. 141/499: loss=7.190505625749985, w0=70.00000000000013, w1=14.655460242304208\n",
      "SubSGD iter. 142/499: loss=0.14080691128663148, w0=69.30000000000013, w1=15.225420366840194\n",
      "SubSGD iter. 143/499: loss=5.847479824352618, w0=70.00000000000013, w1=15.192991359072833\n",
      "SubSGD iter. 144/499: loss=4.728648521097227, w0=70.70000000000013, w1=15.336321949667349\n",
      "SubSGD iter. 145/499: loss=2.1292425166865954, w0=70.00000000000013, w1=15.68340593528734\n",
      "SubSGD iter. 146/499: loss=0.32465205636373184, w0=69.30000000000013, w1=15.348177879984085\n",
      "SubSGD iter. 147/499: loss=11.525306316269479, w0=70.00000000000013, w1=15.55934227455029\n",
      "SubSGD iter. 148/499: loss=5.053805542142754, w0=69.30000000000013, w1=16.239941452206374\n",
      "SubSGD iter. 149/499: loss=1.8533431831531857, w0=70.00000000000013, w1=16.853855865583956\n",
      "SubSGD iter. 150/499: loss=2.642402361603473, w0=69.30000000000013, w1=16.675503744204143\n",
      "SubSGD iter. 151/499: loss=0.3078830396848673, w0=70.00000000000013, w1=17.42148939779154\n",
      "SubSGD iter. 152/499: loss=8.009890095097717, w0=70.70000000000013, w1=17.68697310001577\n",
      "SubSGD iter. 153/499: loss=7.209202100602454, w0=71.40000000000013, w1=17.95245680224\n",
      "SubSGD iter. 154/499: loss=0.9240798259013587, w0=72.10000000000014, w1=17.842282685359258\n",
      "SubSGD iter. 155/499: loss=0.8886134979437017, w0=71.40000000000013, w1=18.26806883265\n",
      "SubSGD iter. 156/499: loss=5.225249941804947, w0=72.10000000000014, w1=17.497245766581813\n",
      "SubSGD iter. 157/499: loss=1.2193581365246402, w0=71.40000000000013, w1=18.2313600895828\n",
      "SubSGD iter. 158/499: loss=0.5414380874031508, w0=70.70000000000013, w1=18.224932208704455\n",
      "SubSGD iter. 159/499: loss=0.33343398542513114, w0=71.40000000000013, w1=18.568644829887845\n",
      "SubSGD iter. 160/499: loss=0.3356712320078188, w0=72.10000000000014, w1=17.760531088845912\n",
      "SubSGD iter. 161/499: loss=7.037297447269239, w0=72.80000000000014, w1=18.999847389823913\n",
      "SubSGD iter. 162/499: loss=1.9900308940333176, w0=73.50000000000014, w1=18.21561466178459\n",
      "SubSGD iter. 163/499: loss=5.547880857924525, w0=72.80000000000014, w1=17.514680664349836\n",
      "SubSGD iter. 164/499: loss=3.352340323850015, w0=72.10000000000014, w1=17.17945260904658\n",
      "SubSGD iter. 165/499: loss=3.6817810156474238, w0=71.40000000000013, w1=17.423479433755734\n",
      "SubSGD iter. 166/499: loss=2.1865550160325853, w0=70.70000000000013, w1=17.83455461829796\n",
      "SubSGD iter. 167/499: loss=13.870249275878024, w0=71.40000000000013, w1=17.21541346288358\n",
      "SubSGD iter. 168/499: loss=1.2257785275775461, w0=70.70000000000013, w1=16.584248122888727\n",
      "SubSGD iter. 169/499: loss=0.9792488731950755, w0=70.00000000000013, w1=16.088621862419895\n",
      "SubSGD iter. 170/499: loss=6.190898062331172, w0=70.70000000000013, w1=14.734664968386667\n",
      "SubSGD iter. 171/499: loss=1.773501653917819, w0=71.40000000000013, w1=15.34857938176425\n",
      "SubSGD iter. 172/499: loss=6.504618593163684, w0=72.10000000000014, w1=15.901785297840362\n",
      "SubSGD iter. 173/499: loss=9.60055050081715, w0=72.80000000000014, w1=16.1460980215495\n",
      "SubSGD iter. 174/499: loss=2.3551258474901218, w0=73.50000000000014, w1=14.934675133623319\n",
      "SubSGD iter. 175/499: loss=0.6078705755490645, w0=74.20000000000014, w1=14.330909063656804\n",
      "SubSGD iter. 176/499: loss=10.448192160913528, w0=73.50000000000014, w1=15.011508241312887\n",
      "SubSGD iter. 177/499: loss=0.29236005162067613, w0=72.80000000000014, w1=13.927632844621584\n",
      "SubSGD iter. 178/499: loss=0.944981498809156, w0=73.50000000000014, w1=14.806805156852391\n",
      "SubSGD iter. 179/499: loss=4.339669428932055, w0=72.80000000000014, w1=14.149622453657406\n",
      "SubSGD iter. 180/499: loss=0.2442888294187071, w0=73.50000000000014, w1=14.131042994758529\n",
      "SubSGD iter. 181/499: loss=0.14470466977175533, w0=74.20000000000014, w1=15.671105333145714\n",
      "SubSGD iter. 182/499: loss=0.36562651703657423, w0=74.90000000000015, w1=14.992758337171171\n",
      "SubSGD iter. 183/499: loss=10.484484498527806, w0=74.20000000000014, w1=15.277932831720465\n",
      "SubSGD iter. 184/499: loss=0.7571700587127879, w0=73.50000000000014, w1=14.543657088228667\n",
      "SubSGD iter. 185/499: loss=7.604256682974679, w0=72.80000000000014, w1=14.760914092408417\n",
      "SubSGD iter. 186/499: loss=1.3767163887122749, w0=72.10000000000014, w1=15.560744826471126\n",
      "SubSGD iter. 187/499: loss=0.19766820070988445, w0=71.40000000000013, w1=14.655569723284062\n",
      "SubSGD iter. 188/499: loss=1.5768718675552975, w0=72.10000000000014, w1=14.555292624043853\n",
      "SubSGD iter. 189/499: loss=6.996930853380107, w0=72.80000000000014, w1=14.820776326268083\n",
      "SubSGD iter. 190/499: loss=5.204038563937736, w0=72.10000000000014, w1=15.064803150977234\n",
      "SubSGD iter. 191/499: loss=1.5517428544604996, w0=72.80000000000014, w1=16.15742766530303\n",
      "SubSGD iter. 192/499: loss=3.098412022260831, w0=72.10000000000014, w1=16.993466522575844\n",
      "SubSGD iter. 193/499: loss=2.7075734550957193, w0=71.40000000000013, w1=17.340550508195836\n",
      "SubSGD iter. 194/499: loss=0.62736175434123, w0=72.10000000000014, w1=16.90086210311065\n",
      "SubSGD iter. 195/499: loss=1.495241467094793, w0=72.80000000000014, w1=16.106492758619872\n",
      "SubSGD iter. 196/499: loss=12.162676801520902, w0=73.50000000000014, w1=16.45096100549034\n",
      "SubSGD iter. 197/499: loss=4.756739411916307, w0=72.80000000000014, w1=16.733508824576564\n",
      "SubSGD iter. 198/499: loss=6.494977844199589, w0=73.50000000000014, w1=16.30317723611583\n",
      "SubSGD iter. 199/499: loss=0.2871034151973646, w0=74.20000000000014, w1=16.20290013687562\n",
      "SubSGD iter. 200/499: loss=1.8673030232296668, w0=74.90000000000015, w1=17.21533847474204\n",
      "SubSGD iter. 201/499: loss=9.122560420774505, w0=75.60000000000015, w1=16.596197319327658\n",
      "SubSGD iter. 202/499: loss=3.36933776948198, w0=74.90000000000015, w1=17.021983466618398\n",
      "SubSGD iter. 203/499: loss=6.301101411556303, w0=75.60000000000015, w1=16.282650667174455\n",
      "SubSGD iter. 204/499: loss=5.560393290603145, w0=76.30000000000015, w1=15.883192894787875\n",
      "SubSGD iter. 205/499: loss=0.10724650206618946, w0=77.00000000000016, w1=16.895871236140643\n",
      "SubSGD iter. 206/499: loss=2.8494152925761256, w0=77.70000000000016, w1=17.17399820494233\n",
      "SubSGD iter. 207/499: loss=6.784279918979067, w0=77.00000000000016, w1=18.010037062215144\n",
      "SubSGD iter. 208/499: loss=5.190903214377073, w0=76.30000000000015, w1=17.28501687213434\n",
      "SubSGD iter. 209/499: loss=12.75200596820659, w0=75.60000000000015, w1=16.736622368376526\n",
      "SubSGD iter. 210/499: loss=10.7278302537471, w0=74.90000000000015, w1=15.789710106634097\n",
      "SubSGD iter. 211/499: loss=1.8508323480789954, w0=75.60000000000015, w1=15.205397569088206\n",
      "SubSGD iter. 212/499: loss=0.27999111830263246, w0=76.30000000000015, w1=15.937362162595841\n",
      "SubSGD iter. 213/499: loss=4.929766666706058, w0=75.60000000000015, w1=16.464319198364688\n",
      "SubSGD iter. 214/499: loss=1.432584926673897, w0=74.90000000000015, w1=15.58514688613388\n",
      "SubSGD iter. 215/499: loss=0.022045605547759806, w0=75.60000000000015, w1=16.464394852030054\n",
      "SubSGD iter. 216/499: loss=0.4308273365087132, w0=74.90000000000015, w1=16.81751060359737\n",
      "SubSGD iter. 217/499: loss=3.0692343098304917, w0=74.20000000000014, w1=16.071524950009973\n",
      "SubSGD iter. 218/499: loss=0.09148890631658446, w0=74.90000000000015, w1=16.341663400299645\n",
      "SubSGD iter. 219/499: loss=2.124674896143439, w0=74.20000000000014, w1=17.23049522446614\n",
      "SubSGD iter. 220/499: loss=3.225278006685258, w0=74.90000000000015, w1=17.843487721649698\n",
      "SubSGD iter. 221/499: loss=2.421539083172661, w0=74.20000000000014, w1=18.45628589265111\n",
      "SubSGD iter. 222/499: loss=0.5364956300182229, w0=74.90000000000015, w1=18.883802977868527\n",
      "SubSGD iter. 223/499: loss=0.20313517111378587, w0=74.20000000000014, w1=18.267416029031278\n",
      "SubSGD iter. 224/499: loss=5.283829270217268, w0=73.50000000000014, w1=17.3876858991988\n",
      "SubSGD iter. 225/499: loss=0.098051417637123, w0=74.20000000000014, w1=18.119650492706434\n",
      "SubSGD iter. 226/499: loss=0.2279044004016697, w0=73.50000000000014, w1=18.85376481570742\n",
      "SubSGD iter. 227/499: loss=8.344418357279132, w0=74.20000000000014, w1=18.329131890995683\n",
      "SubSGD iter. 228/499: loss=0.9852819986659256, w0=73.50000000000014, w1=17.712744942158434\n",
      "SubSGD iter. 229/499: loss=0.0453596099327811, w0=72.80000000000014, w1=18.44685926515942\n",
      "SubSGD iter. 230/499: loss=0.1482507227115093, w0=72.10000000000014, w1=18.33655923169863\n",
      "SubSGD iter. 231/499: loss=3.278402365682119, w0=71.40000000000013, w1=18.580586056407782\n",
      "SubSGD iter. 232/499: loss=1.27331817935233, w0=72.10000000000014, w1=18.562006597508905\n",
      "SubSGD iter. 233/499: loss=0.9170941744304457, w0=72.80000000000014, w1=19.441254563405078\n",
      "SubSGD iter. 234/499: loss=3.6781127434282226, w0=73.50000000000014, w1=18.6468852189143\n",
      "SubSGD iter. 235/499: loss=1.8890888635081424, w0=74.20000000000014, w1=17.84705448485159\n",
      "SubSGD iter. 236/499: loss=0.14516500221739648, w0=73.50000000000014, w1=17.865633943750467\n",
      "SubSGD iter. 237/499: loss=0.06413920125321226, w0=74.20000000000014, w1=18.489122066036643\n",
      "SubSGD iter. 238/499: loss=8.395258131283555, w0=74.90000000000015, w1=18.106996081486308\n",
      "SubSGD iter. 239/499: loss=1.9382855439842928, w0=75.60000000000015, w1=16.981063667985136\n",
      "SubSGD iter. 240/499: loss=2.5769270283888517, w0=76.30000000000015, w1=17.246547370209367\n",
      "SubSGD iter. 241/499: loss=2.0795887147029077, w0=77.00000000000016, w1=18.259225711562134\n",
      "SubSGD iter. 242/499: loss=3.8368326993964104, w0=76.30000000000015, w1=18.207065366032424\n",
      "SubSGD iter. 243/499: loss=0.1254362559321578, w0=75.60000000000015, w1=17.488902042001236\n",
      "SubSGD iter. 244/499: loss=0.7986385308796571, w0=74.90000000000015, w1=16.476463704134815\n",
      "SubSGD iter. 245/499: loss=3.2812174975853594, w0=75.60000000000015, w1=17.139608021177864\n",
      "SubSGD iter. 246/499: loss=2.2482268317803005, w0=74.90000000000015, w1=17.947721762219796\n",
      "SubSGD iter. 247/499: loss=1.8718206822489947, w0=75.60000000000015, w1=17.269374766245253\n",
      "SubSGD iter. 248/499: loss=0.9609564065865186, w0=74.90000000000015, w1=16.80279264027868\n",
      "SubSGD iter. 249/499: loss=1.7634577354736933, w0=74.20000000000014, w1=16.188878226901096\n",
      "SubSGD iter. 250/499: loss=1.0291614473659791, w0=74.90000000000015, w1=14.977455338974917\n",
      "SubSGD iter. 251/499: loss=3.70540091725654, w0=74.20000000000014, w1=14.276521341540164\n",
      "SubSGD iter. 252/499: loss=8.769749302999529, w0=74.90000000000015, w1=14.624488387930578\n",
      "SubSGD iter. 253/499: loss=0.3065446898161781, w0=75.60000000000015, w1=15.419658113047925\n",
      "SubSGD iter. 254/499: loss=3.995400098085014, w0=74.90000000000015, w1=16.308489937214418\n",
      "SubSGD iter. 255/499: loss=0.4635350436456065, w0=74.20000000000014, w1=15.929950153857686\n",
      "SubSGD iter. 256/499: loss=6.449806902001001, w0=74.90000000000015, w1=16.14111454842389\n",
      "SubSGD iter. 257/499: loss=1.1930775541121363, w0=75.60000000000015, w1=16.78726178181111\n",
      "SubSGD iter. 258/499: loss=9.430723616395412, w0=74.90000000000015, w1=17.219054360094532\n",
      "SubSGD iter. 259/499: loss=8.340093141332382, w0=74.20000000000014, w1=17.899653537750616\n",
      "SubSGD iter. 260/499: loss=10.427828447601073, w0=74.90000000000015, w1=17.280512382336234\n",
      "SubSGD iter. 261/499: loss=8.920473704837377, w0=74.20000000000014, w1=16.85697112928222\n",
      "SubSGD iter. 262/499: loss=4.825877658763616, w0=73.50000000000014, w1=17.29716638398919\n",
      "SubSGD iter. 263/499: loss=6.235304814351551, w0=72.80000000000014, w1=17.775214586415313\n",
      "SubSGD iter. 264/499: loss=1.2794609695062888, w0=73.50000000000014, w1=18.20273167163273\n",
      "SubSGD iter. 265/499: loss=0.08388362693268903, w0=74.20000000000014, w1=18.452330134319908\n",
      "SubSGD iter. 266/499: loss=7.35151283242724, w0=73.50000000000014, w1=18.73225912675637\n",
      "SubSGD iter. 267/499: loss=5.303440062175767, w0=72.80000000000014, w1=17.82708402356931\n",
      "SubSGD iter. 268/499: loss=6.5707145835657315, w0=72.10000000000014, w1=17.85238178895815\n",
      "SubSGD iter. 269/499: loss=0.7635607257493646, w0=72.80000000000014, w1=17.033723419152523\n",
      "SubSGD iter. 270/499: loss=3.9080747838540475, w0=73.50000000000014, w1=16.77829153805095\n",
      "SubSGD iter. 271/499: loss=8.226129894388919, w0=74.20000000000014, w1=17.126258584441366\n",
      "SubSGD iter. 272/499: loss=3.666492394679949, w0=74.90000000000015, w1=16.541946046895475\n",
      "SubSGD iter. 273/499: loss=6.204235729866475, w0=74.20000000000014, w1=16.953021231437702\n",
      "SubSGD iter. 274/499: loss=3.6971994102121357, w0=73.50000000000014, w1=16.360703270169523\n",
      "SubSGD iter. 275/499: loss=6.56205049854087, w0=74.20000000000014, w1=16.63883023897121\n",
      "SubSGD iter. 276/499: loss=0.2894661887330159, w0=73.50000000000014, w1=17.295711206724175\n",
      "SubSGD iter. 277/499: loss=3.9577101842290645, w0=72.80000000000014, w1=17.642795192344167\n",
      "SubSGD iter. 278/499: loss=2.318764128818181, w0=72.10000000000014, w1=18.178314875170372\n",
      "SubSGD iter. 279/499: loss=0.2893584285391171, w0=71.40000000000013, w1=17.949193182883615\n",
      "SubSGD iter. 280/499: loss=3.536764079616887, w0=72.10000000000014, w1=18.473373734335333\n",
      "SubSGD iter. 281/499: loss=0.34330462137311457, w0=72.80000000000014, w1=17.930357272970234\n",
      "SubSGD iter. 282/499: loss=1.2446880686058677, w0=72.10000000000014, w1=17.135187547852887\n",
      "SubSGD iter. 283/499: loss=3.0424926580251395, w0=72.80000000000014, w1=16.438602796612123\n",
      "SubSGD iter. 284/499: loss=1.4315280727398658, w0=73.50000000000014, w1=17.2719403791717\n",
      "SubSGD iter. 285/499: loss=0.21908230694708664, w0=74.20000000000014, w1=18.003904972679333\n",
      "SubSGD iter. 286/499: loss=4.645699965281111, w0=73.50000000000014, w1=18.41498015722156\n",
      "SubSGD iter. 287/499: loss=4.349255487543701, w0=74.20000000000014, w1=18.078617199545484\n",
      "SubSGD iter. 288/499: loss=1.515700634596861, w0=73.50000000000014, w1=18.691415370546896\n",
      "SubSGD iter. 289/499: loss=0.5902843438703229, w0=72.80000000000014, w1=18.581115337086104\n",
      "SubSGD iter. 290/499: loss=5.651162277763831, w0=72.10000000000014, w1=18.798372341265853\n",
      "SubSGD iter. 291/499: loss=0.7347151170013859, w0=71.40000000000013, w1=19.33389202409206\n",
      "SubSGD iter. 292/499: loss=2.819372151135731, w0=72.10000000000014, w1=18.712840450521465\n",
      "SubSGD iter. 293/499: loss=8.619441156748408, w0=72.80000000000014, w1=18.9571531742306\n",
      "SubSGD iter. 294/499: loss=0.6655683558871885, w0=72.10000000000014, w1=18.728031481943844\n",
      "SubSGD iter. 295/499: loss=0.02282710471966709, w0=72.80000000000014, w1=19.740709823296612\n",
      "SubSGD iter. 296/499: loss=0.2887595556761653, w0=73.50000000000014, w1=18.93259608225468\n",
      "SubSGD iter. 297/499: loss=7.142742446835314, w0=74.20000000000014, w1=19.176908805963816\n",
      "SubSGD iter. 298/499: loss=10.80098802723235, w0=74.90000000000015, w1=18.149976608371283\n",
      "SubSGD iter. 299/499: loss=0.8961003491580257, w0=74.20000000000014, w1=18.88409093137227\n",
      "SubSGD iter. 300/499: loss=0.29460783196435614, w0=73.50000000000014, w1=18.994265048253013\n",
      "SubSGD iter. 301/499: loss=1.477164850555937, w0=74.20000000000014, w1=19.190785131031515\n",
      "SubSGD iter. 302/499: loss=8.778291973474651, w0=74.90000000000015, w1=18.80865914648118\n",
      "SubSGD iter. 303/499: loss=1.3905799759934183, w0=74.20000000000014, w1=17.795980805128412\n",
      "SubSGD iter. 304/499: loss=0.0026660756855392265, w0=73.50000000000014, w1=17.000811080011065\n",
      "SubSGD iter. 305/499: loss=1.0524191549921227, w0=74.20000000000014, w1=17.428328165228482\n",
      "SubSGD iter. 306/499: loss=0.32429265680355, w0=73.50000000000014, w1=17.225891275187482\n",
      "SubSGD iter. 307/499: loss=2.426576813259814, w0=74.20000000000014, w1=17.24119894189962\n",
      "SubSGD iter. 308/499: loss=3.4690725853156437, w0=74.90000000000015, w1=16.668188858884996\n",
      "SubSGD iter. 309/499: loss=5.877274543247182, w0=74.20000000000014, w1=17.29097526570529\n",
      "SubSGD iter. 310/499: loss=1.2373270874654594, w0=73.50000000000014, w1=16.638918368262832\n",
      "SubSGD iter. 311/499: loss=6.451508895815792, w0=74.20000000000014, w1=16.91704533706452\n",
      "SubSGD iter. 312/499: loss=1.7494853061564157, w0=73.50000000000014, w1=18.006840360137737\n",
      "SubSGD iter. 313/499: loss=1.5526563891332472, w0=74.20000000000014, w1=17.65372460857042\n",
      "SubSGD iter. 314/499: loss=0.15029638006066648, w0=73.50000000000014, w1=17.672304067469298\n",
      "SubSGD iter. 315/499: loss=0.17177571412111092, w0=74.20000000000014, w1=18.286965048599686\n",
      "SubSGD iter. 316/499: loss=0.552909365041387, w0=73.50000000000014, w1=17.274526710733266\n",
      "SubSGD iter. 317/499: loss=4.363918769954267, w0=72.80000000000014, w1=16.91949923320344\n",
      "SubSGD iter. 318/499: loss=0.5033579889315405, w0=73.50000000000014, w1=17.29803901656017\n",
      "SubSGD iter. 319/499: loss=0.6695121307033673, w0=74.20000000000014, w1=16.35453253302305\n",
      "SubSGD iter. 320/499: loss=2.7432600245204384, w0=74.90000000000015, w1=15.781522450008428\n",
      "SubSGD iter. 321/499: loss=3.968642276408559, w0=74.20000000000014, w1=15.965683228326748\n",
      "SubSGD iter. 322/499: loss=2.80844668835266, w0=73.50000000000014, w1=16.492640264095595\n",
      "SubSGD iter. 323/499: loss=5.594280212284673, w0=72.80000000000014, w1=15.324309463578127\n",
      "SubSGD iter. 324/499: loss=0.5451124830318719, w0=72.10000000000014, w1=15.434483580458867\n",
      "SubSGD iter. 325/499: loss=0.07428890709925895, w0=71.40000000000013, w1=15.392157023855376\n",
      "SubSGD iter. 326/499: loss=4.566677088628424, w0=72.10000000000014, w1=15.407464690567512\n",
      "SubSGD iter. 327/499: loss=1.1286712397366898, w0=71.40000000000013, w1=15.93442172633636\n",
      "SubSGD iter. 328/499: loss=1.3718680234256908, w0=70.70000000000013, w1=15.188436072748964\n",
      "SubSGD iter. 329/499: loss=4.566074436142287, w0=70.00000000000013, w1=15.213733838137806\n",
      "SubSGD iter. 330/499: loss=3.924097963515365, w0=70.70000000000013, w1=16.008903563255153\n",
      "SubSGD iter. 331/499: loss=0.29177432517921886, w0=71.40000000000013, w1=16.259184679717208\n",
      "SubSGD iter. 332/499: loss=8.60205774016248, w0=72.10000000000014, w1=16.01820476505056\n",
      "SubSGD iter. 333/499: loss=3.897434073875914, w0=72.80000000000014, w1=17.030883106403326\n",
      "SubSGD iter. 334/499: loss=0.517142633049076, w0=73.50000000000014, w1=16.930606007163117\n",
      "SubSGD iter. 335/499: loss=2.3817819221912657, w0=72.80000000000014, w1=16.66582089797235\n",
      "SubSGD iter. 336/499: loss=1.635995004347535, w0=72.10000000000014, w1=16.849981676290668\n",
      "SubSGD iter. 337/499: loss=0.30383853038161135, w0=71.40000000000013, w1=16.10399602270327\n",
      "SubSGD iter. 338/499: loss=3.585126658246182, w0=72.10000000000014, w1=15.425649026728728\n",
      "SubSGD iter. 339/499: loss=3.6259650837653936, w0=71.40000000000013, w1=15.865844281435699\n",
      "SubSGD iter. 340/499: loss=0.928021410592855, w0=70.70000000000013, w1=16.684502651241328\n",
      "SubSGD iter. 341/499: loss=0.9157834995610017, w0=70.00000000000013, w1=15.62658734441377\n",
      "SubSGD iter. 342/499: loss=3.4551142362308127, w0=70.70000000000013, w1=16.421757069531118\n",
      "SubSGD iter. 343/499: loss=14.107536023691736, w0=71.40000000000013, w1=16.766225316401584\n",
      "SubSGD iter. 344/499: loss=4.226871303998458, w0=72.10000000000014, w1=16.08787832042704\n",
      "SubSGD iter. 345/499: loss=1.2019098653955211, w0=72.80000000000014, w1=16.812898510507846\n",
      "SubSGD iter. 346/499: loss=0.8172949753967487, w0=73.50000000000014, w1=16.191846936937253\n",
      "SubSGD iter. 347/499: loss=3.004979581469456, w0=74.20000000000014, w1=15.637739862940672\n",
      "SubSGD iter. 348/499: loss=8.427194381787615, w0=74.90000000000015, w1=15.018598707526289\n",
      "SubSGD iter. 349/499: loss=0.039273392240602334, w0=75.60000000000015, w1=15.670655604968744\n",
      "SubSGD iter. 350/499: loss=4.484931720222249, w0=74.90000000000015, w1=15.871405142017995\n",
      "SubSGD iter. 351/499: loss=3.099523424871329, w0=74.20000000000014, w1=16.711432713477805\n",
      "SubSGD iter. 352/499: loss=0.26390473846549867, w0=74.90000000000015, w1=17.33492083576398\n",
      "SubSGD iter. 353/499: loss=0.6906350728395694, w0=74.20000000000014, w1=16.2166106151525\n",
      "SubSGD iter. 354/499: loss=1.5104036350245877, w0=74.90000000000015, w1=17.22928895650527\n",
      "SubSGD iter. 355/499: loss=0.04585203857463682, w0=74.20000000000014, w1=17.85034053007586\n",
      "SubSGD iter. 356/499: loss=12.442137940368234, w0=73.50000000000014, w1=17.794030915030557\n",
      "SubSGD iter. 357/499: loss=0.6468180843825664, w0=74.20000000000014, w1=17.172979341459964\n",
      "SubSGD iter. 358/499: loss=3.7204828002840884, w0=73.50000000000014, w1=17.66266287485655\n",
      "SubSGD iter. 359/499: loss=0.01090631070569259, w0=72.80000000000014, w1=17.1534770309103\n",
      "SubSGD iter. 360/499: loss=8.857333804022332, w0=73.50000000000014, w1=16.75401925852372\n",
      "SubSGD iter. 361/499: loss=3.9779257647976323, w0=74.20000000000014, w1=15.400062364490491\n",
      "SubSGD iter. 362/499: loss=0.9545244472246566, w0=74.90000000000015, w1=16.279310330386664\n",
      "SubSGD iter. 363/499: loss=2.6100211323410605, w0=74.20000000000014, w1=17.119337901846475\n",
      "SubSGD iter. 364/499: loss=2.669471291037027, w0=73.50000000000014, w1=17.320087438895726\n",
      "SubSGD iter. 365/499: loss=3.4201593642322905, w0=72.80000000000014, w1=16.688922098900875\n",
      "SubSGD iter. 366/499: loss=5.008090244830413, w0=73.50000000000014, w1=16.726209410518745\n",
      "SubSGD iter. 367/499: loss=3.9038357864437785, w0=74.20000000000014, w1=17.90701411121864\n",
      "SubSGD iter. 368/499: loss=5.4754348633226755, w0=74.90000000000015, w1=18.103358704735868\n",
      "SubSGD iter. 369/499: loss=0.1807724147621741, w0=75.60000000000015, w1=19.284163405435763\n",
      "SubSGD iter. 370/499: loss=5.07176744945243, w0=74.90000000000015, w1=19.631247391055755\n",
      "SubSGD iter. 371/499: loss=10.342806920824529, w0=74.20000000000014, w1=19.20770613800174\n",
      "SubSGD iter. 372/499: loss=1.6832382969667776, w0=74.90000000000015, w1=19.223013804713876\n",
      "SubSGD iter. 373/499: loss=3.46217276607031, w0=74.20000000000014, w1=18.3896762221543\n",
      "SubSGD iter. 374/499: loss=0.8031136539606933, w0=74.90000000000015, w1=18.65981467244397\n",
      "SubSGD iter. 375/499: loss=1.21282208795013, w0=75.60000000000015, w1=19.32295898948702\n",
      "SubSGD iter. 376/499: loss=1.9775245380559454, w0=74.90000000000015, w1=19.157152729919524\n",
      "SubSGD iter. 377/499: loss=0.7329326991227845, w0=75.60000000000015, w1=18.804036978352208\n",
      "SubSGD iter. 378/499: loss=8.381974723551977, w0=74.90000000000015, w1=19.021293982531958\n",
      "SubSGD iter. 379/499: loss=10.914513399286264, w0=74.20000000000014, w1=18.760581761150092\n",
      "SubSGD iter. 380/499: loss=1.1953121126741308, w0=74.90000000000015, w1=18.577115982099674\n",
      "SubSGD iter. 381/499: loss=0.8294238323069862, w0=74.20000000000014, w1=19.23399694985264\n",
      "SubSGD iter. 382/499: loss=13.516148720202608, w0=73.50000000000014, w1=18.275861858950833\n",
      "SubSGD iter. 383/499: loss=4.435910324073731, w0=74.20000000000014, w1=19.400991819081142\n",
      "SubSGD iter. 384/499: loss=0.5100683511789583, w0=73.50000000000014, w1=20.104496737472004\n",
      "SubSGD iter. 385/499: loss=1.0298840281985377, w0=74.20000000000014, w1=19.44761576971904\n",
      "SubSGD iter. 386/499: loss=0.6569339632432332, w0=74.90000000000015, w1=19.320601324491015\n",
      "SubSGD iter. 387/499: loss=8.481022025896621, w0=75.60000000000015, w1=19.66506957136148\n",
      "SubSGD iter. 388/499: loss=2.843704473769023, w0=74.90000000000015, w1=19.55476953790069\n",
      "SubSGD iter. 389/499: loss=3.142133631424123, w0=75.60000000000015, w1=18.7839464718325\n",
      "SubSGD iter. 390/499: loss=9.258339214895116, w0=74.90000000000015, w1=17.727545240453612\n",
      "SubSGD iter. 391/499: loss=1.8483240932909553, w0=74.20000000000014, w1=18.077882173822005\n",
      "SubSGD iter. 392/499: loss=2.3433996452735926, w0=73.50000000000014, w1=17.629223135037293\n",
      "SubSGD iter. 393/499: loss=8.342461702964243, w0=74.20000000000014, w1=16.88989033559335\n",
      "SubSGD iter. 394/499: loss=0.934796976029844, w0=73.50000000000014, w1=16.83772999006364\n",
      "SubSGD iter. 395/499: loss=5.200908852278424, w0=72.80000000000014, w1=17.081756814772792\n",
      "SubSGD iter. 396/499: loss=4.059386498373328, w0=72.10000000000014, w1=17.26362065834796\n",
      "SubSGD iter. 397/499: loss=0.27849433880065533, w0=71.40000000000013, w1=16.81496161956325\n",
      "SubSGD iter. 398/499: loss=0.43348212662763785, w0=70.70000000000013, w1=16.068975965975852\n",
      "SubSGD iter. 399/499: loss=3.0735112251297068, w0=71.40000000000013, w1=16.715123199363074\n",
      "SubSGD iter. 400/499: loss=3.50839371209112, w0=72.10000000000014, w1=17.18170532532965\n",
      "SubSGD iter. 401/499: loss=3.1968158191873286, w0=72.80000000000014, w1=15.997703124935757\n",
      "SubSGD iter. 402/499: loss=0.027649949556845854, w0=73.50000000000014, w1=16.200140014976757\n",
      "SubSGD iter. 403/499: loss=3.8190090578313516, w0=72.80000000000014, w1=15.845112537446932\n",
      "SubSGD iter. 404/499: loss=0.7249884472464743, w0=73.50000000000014, w1=16.67845012000651\n",
      "SubSGD iter. 405/499: loss=4.416604066700025, w0=74.20000000000014, w1=15.82460384958124\n",
      "SubSGD iter. 406/499: loss=0.4258100057872696, w0=74.90000000000015, w1=15.641138070530822\n",
      "SubSGD iter. 407/499: loss=1.7268144860449013, w0=75.60000000000015, w1=15.05682553298493\n",
      "SubSGD iter. 408/499: loss=9.865291838129274, w0=74.90000000000015, w1=15.534873735411054\n",
      "SubSGD iter. 409/499: loss=3.3149047406600474, w0=74.20000000000014, w1=15.960659882701794\n",
      "SubSGD iter. 410/499: loss=2.209261389892177, w0=73.50000000000014, w1=16.768773623743726\n",
      "SubSGD iter. 411/499: loss=1.1341501532792577, w0=72.80000000000014, w1=17.20846202882891\n",
      "SubSGD iter. 412/499: loss=1.7835161397661707, w0=72.10000000000014, w1=17.713479760653232\n",
      "SubSGD iter. 413/499: loss=1.5806643471377981, w0=71.40000000000013, w1=16.34811030798962\n",
      "SubSGD iter. 414/499: loss=8.667053987682777, w0=72.10000000000014, w1=16.626237276791308\n",
      "SubSGD iter. 415/499: loss=3.827432303047985, w0=72.80000000000014, w1=17.150417828243025\n",
      "SubSGD iter. 416/499: loss=1.3402361757770223, w0=73.50000000000014, w1=16.784497097512627\n",
      "SubSGD iter. 417/499: loss=1.124965225258876, w0=74.20000000000014, w1=15.600494897118736\n",
      "SubSGD iter. 418/499: loss=9.332316550266853, w0=73.50000000000014, w1=15.052100393360925\n",
      "SubSGD iter. 419/499: loss=0.9875279194484108, w0=74.20000000000014, w1=15.502280139567219\n",
      "SubSGD iter. 420/499: loss=3.7710929749366287, w0=73.50000000000014, w1=16.11507831056863\n",
      "SubSGD iter. 421/499: loss=10.14946844292497, w0=72.80000000000014, w1=15.509675159124646\n",
      "SubSGD iter. 422/499: loss=8.579157846827961, w0=72.10000000000014, w1=15.781155341107224\n",
      "SubSGD iter. 423/499: loss=0.8842236471546698, w0=72.80000000000014, w1=16.506175531188028\n",
      "SubSGD iter. 424/499: loss=8.55807357559462, w0=73.50000000000014, w1=17.745491832166028\n",
      "SubSGD iter. 425/499: loss=1.4670726627767863, w0=72.80000000000014, w1=17.21743601665538\n",
      "SubSGD iter. 426/499: loss=5.53503849193946, w0=72.10000000000014, w1=17.039083895275567\n",
      "SubSGD iter. 427/499: loss=2.031844860564135, w0=71.40000000000013, w1=15.955208498584266\n",
      "SubSGD iter. 428/499: loss=0.9135377676780365, w0=72.10000000000014, w1=15.155377764521557\n",
      "SubSGD iter. 429/499: loss=7.021857224322559, w0=72.80000000000014, w1=14.468683129015421\n",
      "SubSGD iter. 430/499: loss=0.6456187446760566, w0=72.10000000000014, w1=14.41652278348571\n",
      "SubSGD iter. 431/499: loss=3.30573153669971, w0=72.80000000000014, w1=15.272744835990249\n",
      "SubSGD iter. 432/499: loss=0.8436972214476697, w0=73.50000000000014, w1=14.06132194806407\n",
      "SubSGD iter. 433/499: loss=1.025128075476644, w0=72.80000000000014, w1=13.442188552119402\n",
      "SubSGD iter. 434/499: loss=0.04998711634030428, w0=73.50000000000014, w1=13.089072800552088\n",
      "SubSGD iter. 435/499: loss=1.8299196972733398, w0=74.20000000000014, w1=14.295089090322536\n",
      "SubSGD iter. 436/499: loss=7.304804789379091, w0=74.90000000000015, w1=15.475893791022433\n",
      "SubSGD iter. 437/499: loss=8.879099227488304, w0=74.20000000000014, w1=15.953941993448556\n",
      "SubSGD iter. 438/499: loss=9.392905836602154, w0=73.50000000000014, w1=16.23911648799785\n",
      "SubSGD iter. 439/499: loss=0.7268133086684259, w0=74.20000000000014, w1=16.509254938287523\n",
      "SubSGD iter. 440/499: loss=11.190376689562953, w0=73.50000000000014, w1=15.903851786843537\n",
      "SubSGD iter. 441/499: loss=8.869972623127055, w0=72.80000000000014, w1=15.355457283085727\n",
      "SubSGD iter. 442/499: loss=0.5500148880418863, w0=72.10000000000014, w1=15.465631399966467\n",
      "SubSGD iter. 443/499: loss=0.29550659977059013, w0=71.40000000000013, w1=14.270892562862041\n",
      "SubSGD iter. 444/499: loss=3.384292383765384, w0=70.70000000000013, w1=14.092540441482226\n",
      "SubSGD iter. 445/499: loss=1.476709932533268, w0=71.40000000000013, w1=14.321662133768985\n",
      "SubSGD iter. 446/499: loss=2.3014772865198836, w0=72.10000000000014, w1=14.700201917125717\n",
      "SubSGD iter. 447/499: loss=8.426908356590431, w0=72.80000000000014, w1=14.318075932575383\n",
      "SubSGD iter. 448/499: loss=2.2536557759481326, w0=72.10000000000014, w1=14.502236710893703\n",
      "SubSGD iter. 449/499: loss=0.7613445960253244, w0=72.80000000000014, w1=14.845949332077094\n",
      "SubSGD iter. 450/499: loss=4.930810376198707, w0=72.10000000000014, w1=14.667597210697279\n",
      "SubSGD iter. 451/499: loss=1.3768226610683882, w0=72.80000000000014, w1=15.500934793256857\n",
      "SubSGD iter. 452/499: loss=0.30218572874700556, w0=72.10000000000014, w1=15.871202883785653\n",
      "SubSGD iter. 453/499: loss=0.227961140241689, w0=71.40000000000013, w1=15.582705619228237\n",
      "SubSGD iter. 454/499: loss=1.02712284203497, w0=70.70000000000013, w1=15.247477563924983\n",
      "SubSGD iter. 455/499: loss=10.28585036719604, w0=70.00000000000013, w1=15.261232759900473\n",
      "SubSGD iter. 456/499: loss=0.15718790560320883, w0=69.30000000000013, w1=14.906205282370648\n",
      "SubSGD iter. 457/499: loss=1.501113373525854, w0=70.00000000000013, w1=14.948531838974139\n",
      "SubSGD iter. 458/499: loss=10.94586490811325, w0=70.70000000000013, w1=15.159696233540343\n",
      "SubSGD iter. 459/499: loss=1.2863546224058666, w0=71.40000000000013, w1=15.211856579070053\n",
      "SubSGD iter. 460/499: loss=1.5996133801496057, w0=72.10000000000014, w1=15.414293469111053\n",
      "SubSGD iter. 461/499: loss=1.5526379536622414, w0=71.40000000000013, w1=16.540225882612223\n",
      "SubSGD iter. 462/499: loss=6.629749331817095, w0=72.10000000000014, w1=17.15321837979578\n",
      "SubSGD iter. 463/499: loss=9.163777608256794, w0=72.80000000000014, w1=17.397531103504917\n",
      "SubSGD iter. 464/499: loss=4.970344352301979, w0=73.50000000000014, w1=17.434818415122788\n",
      "SubSGD iter. 465/499: loss=0.4998410710024217, w0=74.20000000000014, w1=17.05923257683622\n",
      "SubSGD iter. 466/499: loss=0.38202711248911214, w0=74.90000000000015, w1=17.67561952567347\n",
      "SubSGD iter. 467/499: loss=2.8612247606739487, w0=74.20000000000014, w1=17.633292969069977\n",
      "SubSGD iter. 468/499: loss=7.040994162873304, w0=73.50000000000014, w1=17.454940847690164\n",
      "SubSGD iter. 469/499: loss=1.824758960275716, w0=72.80000000000014, w1=17.166443583132747\n",
      "SubSGD iter. 470/499: loss=0.33566832649434986, w0=72.10000000000014, w1=17.86994850152361\n",
      "SubSGD iter. 471/499: loss=5.171885100858546, w0=71.40000000000013, w1=18.08720550570336\n",
      "SubSGD iter. 472/499: loss=5.02608762149594, w0=72.10000000000014, w1=17.31638243963517\n",
      "SubSGD iter. 473/499: loss=4.160704619519905, w0=72.80000000000014, w1=17.481835231567796\n",
      "SubSGD iter. 474/499: loss=9.245386026503553, w0=73.50000000000014, w1=17.09970924701746\n",
      "SubSGD iter. 475/499: loss=7.274280651717589, w0=74.20000000000014, w1=16.749771315537146\n",
      "SubSGD iter. 476/499: loss=2.775457464051982, w0=73.50000000000014, w1=16.950520852586397\n",
      "SubSGD iter. 477/499: loss=0.36033112205691964, w0=74.20000000000014, w1=17.602577750028853\n",
      "SubSGD iter. 478/499: loss=2.0849308971498886, w0=74.90000000000015, w1=16.625780271086935\n",
      "SubSGD iter. 479/499: loss=1.1569627794441715, w0=74.20000000000014, w1=15.97372337364448\n",
      "SubSGD iter. 480/499: loss=1.452102121769947, w0=73.50000000000014, w1=16.34399146417328\n",
      "SubSGD iter. 481/499: loss=7.494634431925782, w0=72.80000000000014, w1=16.623920456609742\n",
      "SubSGD iter. 482/499: loss=1.5657736836333953, w0=72.10000000000014, w1=16.359135347418974\n",
      "SubSGD iter. 483/499: loss=6.932118164485395, w0=72.80000000000014, w1=15.321621153605433\n",
      "SubSGD iter. 484/499: loss=8.763237780980248, w0=72.10000000000014, w1=14.716218002161447\n",
      "SubSGD iter. 485/499: loss=95.29586540853043, w0=72.80000000000014, w1=11.34307432457177\n",
      "SubSGD iter. 486/499: loss=6.893293670731424, w0=72.10000000000014, w1=11.783269579278741\n",
      "SubSGD iter. 487/499: loss=7.358386151148338, w0=72.80000000000014, w1=12.515234172786377\n",
      "SubSGD iter. 488/499: loss=3.477009955913765, w0=72.10000000000014, w1=12.871959691142214\n",
      "SubSGD iter. 489/499: loss=3.7303650063257834, w0=71.40000000000013, w1=13.760791515308707\n",
      "SubSGD iter. 490/499: loss=0.6830713961315382, w0=72.10000000000014, w1=14.025576624499475\n",
      "SubSGD iter. 491/499: loss=3.473403454304858, w0=72.80000000000014, w1=12.988062430685934\n",
      "SubSGD iter. 492/499: loss=2.1203467694712543, w0=72.10000000000014, w1=13.338399364054325\n",
      "SubSGD iter. 493/499: loss=3.339230127952021, w0=71.40000000000013, w1=14.178426935514135\n",
      "SubSGD iter. 494/499: loss=4.832735383684415, w0=70.70000000000013, w1=14.686928053253968\n",
      "SubSGD iter. 495/499: loss=3.82252655088152, w0=71.40000000000013, w1=15.482097778371315\n",
      "SubSGD iter. 496/499: loss=2.219248134105527, w0=72.10000000000014, w1=16.277267503488662\n",
      "SubSGD iter. 497/499: loss=6.121317247762654, w0=71.40000000000013, w1=16.557196495925126\n",
      "SubSGD iter. 498/499: loss=3.4261797920903874, w0=72.10000000000014, w1=15.580399016983208\n",
      "SubSGD iter. 499/499: loss=8.228032427535112, w0=72.80000000000014, w1=15.776743610500436\n",
      "SubSGD: execution time=0.085 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6fd1c7e5ef4b6ea268728f47067ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
